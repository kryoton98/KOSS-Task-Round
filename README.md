# ğŸš€ **KOSS Task Round: Optimizing Matrix Multiplication**

## ğŸ“Œ **Overview**
This repository contains the complete code and resources for benchmarking matrix multiplication on both CPU and GPU, using various implementations and optimizations. The project demonstrates the performance benefits of GPU acceleration using **CuPy** with **FP32** and **TensorCore** support, alongside CPU-based approaches with **NumPy**, **Numba**, and custom CUDA kernels.


---

## âš™ï¸ **Features**
- **CPU Implementations:**  
  - NaÃ¯ve Python multiplication  
  - NumPy-based matrix multiplication  
  - Numba parallel-accelerated multiplication  

- **GPU Implementations:**  
  - **CuPy FP32:** Optimized with standard floating-point precision  
  - **CuPy TensorCore:** Faster, mixed-precision matrix multiplication  
  - Custom CUDA kernel for small matrices with shared memory optimization  

- **Performance Benchmarking:**  
  - Execution time, throughput, and speedup measurements  
  - Comparison against NumPy baseline performance  

- **Visualizations:**  
  - Logarithmic scale performance graphs  
  - Speedup and time comparisons  

---

## ğŸš¦ **How to Run**

1. **Install dependencies:**  
Ensure you have the required libraries installed. Use the following command:  
```bash
pip install numpy numba cupy matplotlib
```

## ğŸ”§ **Customization**
- **Modify matrix sizes** and block dimensions in the scripts for different benchmarks.  
- **Tune the CUDA kernel parameters** to optimize performance for specific hardware.  
- **Experiment with different matrix sizes** to observe scaling behavior.  

## âœ… **Contribute**
Feel free to contribute by suggesting further optimizations, adding new algorithms, or improving the visualizations. ğŸš€

## ğŸ“š **References**
- [CuPy Documentation](https://docs.cupy.dev/)  
- [Numba Documentation](https://numba.readthedocs.io/)  
- [CUDA Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html)  
